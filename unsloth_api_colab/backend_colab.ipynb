{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51cef6d1",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Cell 1: Installation & Setup\n",
    "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "!pip install --no-deps xformers \"trl<0.9.0\" peft accelerate bitsandbytes\n",
    "!pip install fastapi uvicorn pyngrok nest_asyncio python-multipart\n",
    "\n",
    "import nest_asyncio\n",
    "from pyngrok import ngrok\n",
    "import uvicorn\n",
    "import zipfile\n",
    "import shutil\n",
    "\n",
    "# PASTE YOUR NGROK AUTHTOKEN HERE\n",
    "# You can get one from https://dashboard.ngrok.com/get-started/your-authtoken\n",
    "NGROK_AUTHTOKEN = \"PASTE_YOUR_AUTHTOKEN_HERE\"\n",
    "\n",
    "ngrok.set_auth_token(NGROK_AUTHTOKEN)\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf3df7ac",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Cell 2: The FastAPI Application\n",
    "from fastapi import FastAPI, File, UploadFile, HTTPException, BackgroundTasks, Form\n",
    "from fastapi.responses import FileResponse\n",
    "from pydantic import BaseModel\n",
    "from typing import Optional, List, Dict, Any\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import json\n",
    "import uuid\n",
    "import asyncio\n",
    "import subprocess\n",
    "from datetime import datetime\n",
    "import logging\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import zipfile\n",
    "\n",
    "# --- Setup for Colab Environment ---\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "app = FastAPI(\n",
    "    title=\"Unsloth Fine-tuning API (on Colab)\",\n",
    "    description=\"API running on Google Colab for fine-tuning models and automated download.\",\n",
    "    version=\"1.1.0\"\n",
    ")\n",
    "\n",
    "# Global state is stored in memory for the Colab runtime\n",
    "training_jobs: Dict[str, Dict[str, Any]] = {}\n",
    "\n",
    "# Use Colab's temporary filesystem\n",
    "UPLOAD_DIR = Path(\"/content/uploads\")\n",
    "MODELS_DIR = Path(\"/content/trained_models\")\n",
    "ZIPPED_MODELS_DIR = Path(\"/content/zipped_models\")\n",
    "UPLOAD_DIR.mkdir(exist_ok=True)\n",
    "MODELS_DIR.mkdir(exist_ok=True)\n",
    "ZIPPED_MODELS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Pydantic Models remain the same\n",
    "class TrainingStatus(BaseModel):\n",
    "    job_id: str; status: str; progress: Optional[float] = None; message: Optional[str] = None\n",
    "    start_time: Optional[datetime] = None; end_time: Optional[datetime] = None\n",
    "    model_path: Optional[str] = None; logs: Optional[List[str]] = None\n",
    "\n",
    "AVAILABLE_MODELS = [\n",
    "    \"unsloth/Llama-3.2-1B-Instruct\", \"unsloth/tinyllama-bnb-4bit\",\n",
    "    \"unsloth/llama-2-7b-bnb-4bit\", \"unsloth/mistral-7b-bnb-4bit\",\n",
    "]\n",
    "\n",
    "# The core training logic and functions (create_training_script, run_training)\n",
    "# are the same as before. They are included here for completeness.\n",
    "\n",
    "def create_training_script(job_data: Dict[str, Any]) -> str:\n",
    "    model_name = job_data[\"model_name\"]\n",
    "    dataset_path = UPLOAD_DIR / job_data[\"dataset_file\"]\n",
    "    params = job_data[\"parameters\"]\n",
    "    job_id = job_data[\"job_id\"]\n",
    "    output_model_dir = f\"/content/trained_models/{job_id}\"\n",
    "    script = f\"\"\"\n",
    "import os, sys, torch, json, re\n",
    "from unsloth import FastLanguageModel\n",
    "from datasets import Dataset\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "def prepare_document_data(file_path, chunk_size=256):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f: document_text = f.read()\n",
    "    sentences = re.split(r'(?<=[.!?])\\\\s+', document_text)\n",
    "    chunks, current_chunk, current_length = [], [], 0\n",
    "    for sentence in sentences:\n",
    "        sentence = sentence.strip()\n",
    "        if not sentence: continue\n",
    "        sentence_length = len(sentence.split())\n",
    "        if current_length + sentence_length > chunk_size and current_chunk:\n",
    "            chunks.append(\" \".join(current_chunk)); current_chunk = [sentence]; current_length = sentence_length\n",
    "        else: current_chunk.append(sentence); current_length += sentence_length\n",
    "    if current_chunk: chunks.append(\" \".join(current_chunk))\n",
    "    return Dataset.from_dict({{\"text\": chunks}})\n",
    "def main():\n",
    "    try:\n",
    "        model_name = \"{model_name}\"\n",
    "        max_seq_length = {params['max_seq_length']}\n",
    "        model, tokenizer = FastLanguageModel.from_pretrained(model_name=model_name, max_seq_length=max_seq_length, dtype=None, load_in_4bit=True)\n",
    "        model = FastLanguageModel.get_peft_model(model, r=8, target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"], lora_alpha=16, lora_dropout=0, bias=\"none\", use_gradient_checkpointing=True, random_state=3407)\n",
    "        dataset = prepare_document_data(r\"{str(dataset_path)}\")\n",
    "        training_args = TrainingArguments(output_dir=\"{output_model_dir}\", num_train_epochs={params['num_train_epochs']}, per_device_train_batch_size={params['per_device_train_batch_size']}, gradient_accumulation_steps={params['gradient_accumulation_steps']}, warmup_steps={params['warmup_steps']}, learning_rate={params['learning_rate']}, fp16=not torch.cuda.is_bf16_supported(), bf16=torch.cuda.is_bf16_supported(), logging_steps={params['logging_steps']}, optim=\"adamw_8bit\", save_strategy=\"steps\", save_steps={params['save_steps']}, save_total_limit=1, report_to=\"none\", seed=3407, disable_tqdm=True)\n",
    "        trainer = SFTTrainer(model=model, tokenizer=tokenizer, train_dataset=dataset, dataset_text_field=\"text\", max_seq_length=max_seq_length, args=training_args)\n",
    "        print(f\"UNSLOTH_TOTAL_STEPS={{trainer.state.max_steps}}\"); sys.stdout.flush()\n",
    "        trainer.train()\n",
    "        model.save_pretrained(\"{output_model_dir}\"); tokenizer.save_pretrained(\"{output_model_dir}\")\n",
    "    except Exception as e:\n",
    "        import traceback; traceback.print_exc(); sys.exit(1)\n",
    "if __name__ == \"__main__\": main()\n",
    "\"\"\"\n",
    "    return script\n",
    "\n",
    "async def run_training(job_id: str, job_data: Dict[str, Any]):\n",
    "    job_data[\"status\"] = \"running\"\n",
    "    script_content = create_training_script(job_data)\n",
    "    script_path = Path(f\"/content/training_script_{job_id}.py\")\n",
    "    total_steps = 0\n",
    "    try:\n",
    "        with open(script_path, \"w\") as f: f.write(script_content)\n",
    "        process = await asyncio.create_subprocess_exec(sys.executable, \"-u\", str(script_path), stdout=asyncio.subprocess.PIPE, stderr=asyncio.subprocess.STDOUT)\n",
    "        async for line in process.stdout:\n",
    "            line_str = line.decode().strip()\n",
    "            if not line_str: continue\n",
    "            job_data[\"logs\"].append(line_str)\n",
    "            logger.info(f\"Job {job_id}: {line_str}\")\n",
    "            if line_str.startswith(\"UNSLOTH_TOTAL_STEPS=\"):\n",
    "                try: total_steps = int(line_str.split('=')[1])\n",
    "                except: pass\n",
    "            if line_str.startswith(\"{\") and line_str.endswith(\"}\"):\n",
    "                try:\n",
    "                    log_data = json.loads(line_str)\n",
    "                    if \"step\" in log_data and total_steps > 0:\n",
    "                        job_data[\"progress\"] = round((log_data[\"step\"] / total_steps) * 100, 2)\n",
    "                except: pass\n",
    "        await process.wait()\n",
    "        if process.returncode == 0:\n",
    "            job_data[\"status\"] = \"completed\"; job_data[\"progress\"] = 100.0; job_data[\"end_time\"] = datetime.now()\n",
    "            job_data[\"model_path\"] = f\"/content/trained_models/{job_id}\"\n",
    "        else:\n",
    "            job_data[\"status\"] = \"failed\"; job_data[\"end_time\"] = datetime.now()\n",
    "    finally:\n",
    "        if script_path.exists(): script_path.unlink()\n",
    "\n",
    "@app.post(\"/train\", status_code=202)\n",
    "async def start_training(background_tasks: BackgroundTasks, model_name: str = Form(...), dataset_file: str = Form(...)):\n",
    "    if model_name not in AVAILABLE_MODELS: raise HTTPException(400, \"Model not available\")\n",
    "    if not (UPLOAD_DIR / dataset_file).exists(): raise HTTPException(404, \"Dataset not found\")\n",
    "    job_id = str(uuid.uuid4())\n",
    "    training_jobs[job_id] = {\"job_id\": job_id, \"status\": \"pending\", \"model_name\": model_name, \"dataset_file\": dataset_file,\n",
    "        \"parameters\": {\"max_seq_length\": 1024, \"learning_rate\": 2e-4, \"num_train_epochs\": 1, \"per_device_train_batch_size\": 2, \"gradient_accumulation_steps\": 4, \"warmup_steps\": 5, \"save_steps\": 50, \"logging_steps\": 1},\n",
    "        \"start_time\": datetime.now(), \"logs\": [], \"progress\": 0.0}\n",
    "    background_tasks.add_task(run_training, job_id, training_jobs[job_id])\n",
    "    return {\"job_id\": job_id, \"status\": \"Training queued\"}\n",
    "\n",
    "@app.post(\"/upload\")\n",
    "async def upload_dataset(file: UploadFile = File(...)):\n",
    "    file_path = UPLOAD_DIR / file.filename\n",
    "    with open(file_path, \"wb\") as buffer: shutil.copyfileobj(file.file, buffer)\n",
    "    return {\"message\": \"File uploaded successfully\", \"filename\": file.filename}\n",
    "\n",
    "@app.get(\"/status/{job_id}\", response_model=TrainingStatus)\n",
    "async def get_training_status(job_id: str):\n",
    "    if job_id not in training_jobs: raise HTTPException(status_code=404, detail=\"Job not found\")\n",
    "    return TrainingStatus(**training_jobs[job_id])\n",
    "\n",
    "@app.get(\"/download/{job_id}\")\n",
    "async def download_model(job_id: str):\n",
    "    \"\"\"Zips the trained model folder and provides it for download.\"\"\"\n",
    "    if job_id not in training_jobs or training_jobs[job_id].get(\"status\") != \"completed\":\n",
    "        raise HTTPException(status_code=404, detail=\"Job not found or not completed.\")\n",
    "\n",
    "    model_path = Path(training_jobs[job_id][\"model_path\"])\n",
    "    zip_path = ZIPPED_MODELS_DIR / f\"{job_id}.zip\"\n",
    "\n",
    "    # Create a zip file of the model directory\n",
    "    with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "        for root, dirs, files in os.walk(model_path):\n",
    "            for file in files:\n",
    "                file_path = os.path.join(root, file)\n",
    "                archive_name = os.path.relpath(file_path, model_path)\n",
    "                zipf.write(file_path, archive_name)\n",
    "\n",
    "    return FileResponse(path=zip_path, media_type='application/zip', filename=f\"model_{job_id}.zip\")\n",
    "\n",
    "# --- Start the server ---\n",
    "public_url = ngrok.connect(8000)\n",
    "print(\"✅ FastAPI server is running.\")\n",
    "print(f\"🚀 Public URL: {public_url.public_url}\")\n",
    "print(f\"📄 API Docs available at: {public_url.public_url}/docs\")\n",
    "uvicorn.run(app, host=\"0.0.0.0\", port=8000)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
