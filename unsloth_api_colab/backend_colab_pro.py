# -*- coding: utf-8 -*-
"""backend_colab_pro.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15p-XSXcOoGuZGsW_aqc4wLQGBfYt2fNM
"""

# Commented out IPython magic to ensure Python compatibility.
# # Cell 1: Installation & Setup
# %%capture
# import os
# if "COLAB_" not in "".join(os.environ.keys()):
#     !pip install unsloth
# else:
#     # Do this only in Colab notebooks! Otherwise use pip install unsloth
#     !pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl triton cut_cross_entropy unsloth_zoo
#     !pip install sentencepiece protobuf "datasets>=3.4.1,<4.0.0" huggingface_hub hf_transfer
#     !pip install --no-deps unsloth
# 
# !pip install --no-deps xformers "trl<0.9.0" peft accelerate bitsandbytes
# !pip install fastapi uvicorn pyngrok nest_asyncio python-multipart
# 
# import nest_asyncio
# from pyngrok import ngrok
# import uvicorn
# import zipfile
# import shutil
# 
# # PASTE YOUR NGROK AUTHTOKEN HERE
# # You can get one from https://dashboard.ngrok.com/get-started/your-authtoken
# NGROK_AUTHTOKEN = "30SyH46UoShRRT227rMufv0ZRli_2xF4uBz4BNH7SjkP5gHGV"
# 
# ngrok.set_auth_token(NGROK_AUTHTOKEN)
# nest_asyncio.apply()

# !pip install fastapi uvicorn pyngrok nest_asyncio python-multipart pypdf markdown-it-py

# Cell 2: The Final, Corrected FastAPI Application (v2.2)
from fastapi import FastAPI, File, UploadFile, HTTPException, BackgroundTasks, Form
from fastapi.responses import FileResponse, JSONResponse
from pydantic import BaseModel
from typing import Optional, List, Dict, Any
import os, sys, re, json, uuid, asyncio, subprocess, shutil, zipfile, logging
from datetime import datetime
from pathlib import Path

# Document parsing libraries
import pypdf
from markdown_it import MarkdownIt

# --- Setup ---
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)
app = FastAPI(title="Unsloth Fine-tuning API Pro", version="2.2.0")

training_jobs: Dict[str, Dict[str, Any]] = {}
loaded_models: Dict[str, Any] = {}

UPLOAD_DIR, MODELS_DIR, ZIPPED_MODELS_DIR = Path("/content/uploads"), Path("/content/trained_models"), Path("/content/zipped_models")
for d in [UPLOAD_DIR, MODELS_DIR, ZIPPED_MODELS_DIR]: d.mkdir(exist_ok=True)

# --- Pydantic Models ---
class TrainingStatus(BaseModel):
    job_id: str; status: str; progress: Optional[float] = None; model_path: Optional[str] = None; logs: Optional[List[str]] = None

class ChatRequest(BaseModel):
    job_id: str; prompt: str; max_new_tokens: int = 150

AVAILABLE_MODELS = [
    "unsloth/Llama-3.2-1B-Instruct", "unsloth/tinyllama-bnb-4bit",
    "unsloth/mistral-7b-v0.3-bnb-4bit", "unsloth/gemma-2-9b-it-bnb-4bit",
]

# --- Core Logic ---
def create_training_script(job_data: Dict[str, Any]) -> str:
    """Generates the Python script for the background training process."""
    model_name = job_data["model_name"]
    dataset_path = UPLOAD_DIR / job_data["dataset_file"]
    output_model_dir = f"/content/trained_models/{job_data['job_id']}"

    # The f-string now correctly includes all necessary imports for the training script.
    return f"""
import os, sys, torch, json, re
from unsloth import FastLanguageModel
from datasets import Dataset
from transformers import TrainingArguments
import pypdf
from markdown_it import MarkdownIt

# --- FIX IS HERE: SFTTrainer is now correctly imported ---
from trl import SFTTrainer

def parse_document(fp_str):
    fp=__import__('pathlib').Path(fp_str);ext=fp.suffix.lower();c=""
    if ext==".pdf": r=pypdf.PdfReader(fp); [c:=c+p.extract_text()+"\\n" for p in r.pages]
    elif ext==".md": md=MarkdownIt();c=re.sub('<[^<]+?>', '', md.render(open(fp, 'r', encoding='utf-8').read()))
    else: c=open(fp, 'r', encoding='utf-8').read()
    return c

def prep_ds(fp,cs=384):
    dt=parse_document(fp);s=re.split(r'(?<=[.!?])\\s+',dt);ch,cc,cl=[],[],0
    for i in s:
        i=i.strip()
        if not i: continue
        sl=len(i.split())
        if cl+sl>cs and cc: ch.append(" ".join(cc));cc=[i];cl=sl
        else: cc.append(i);cl+=sl
    if cc: ch.append(" ".join(cc))
    return Dataset.from_dict({{"text": ch}})

def main():
    try:
        m,t=FastLanguageModel.from_pretrained(model_name="{model_name}",max_seq_length=2048,load_in_4bit=True)
        m=FastLanguageModel.get_peft_model(m,r=16,lora_alpha=32,target_modules=["q_proj","k_proj","v_proj","o_proj","gate_proj","up_proj","down_proj"])
        d=prep_ds(r"{str(dataset_path)}")
        tr=SFTTrainer(model=m,tokenizer=t,train_dataset=d,dataset_text_field="text",max_seq_length=2048,
            args=TrainingArguments(output_dir="{output_model_dir}",per_device_train_batch_size=2,gradient_accumulation_steps=4,
            warmup_steps=10,num_train_epochs=1,learning_rate=2e-4,fp16=not torch.cuda.is_bf16_supported(),
            bf16=torch.cuda.is_bf16_supported(),logging_steps=1,optim="adamw_8bit",seed=3407,report_to="none",disable_tqdm=True))
        print(f"UNSLOTH_TOTAL_STEPS={{tr.state.max_steps}}");sys.stdout.flush();tr.train();m.save_pretrained("{output_model_dir}")
    except Exception as e:import traceback;traceback.print_exc();sys.exit(1)

if __name__=="__main__":main()
"""

async def run_training(job_id: str, job_data: Dict[str, Any]):
    job_data["status"]="running"
    script_path=Path(f"/content/training_script_{job_id}.py")
    with open(script_path,"w") as f: f.write(create_training_script(job_data))
    proc=await asyncio.create_subprocess_exec(sys.executable,"-u",str(script_path),stdout=asyncio.subprocess.PIPE,stderr=asyncio.subprocess.STDOUT)
    ts=0
    # Ensure 'logs' list exists before appending
    job_data.setdefault("logs", [])
    async for l in proc.stdout:
        ls=l.decode().strip()
        if not ls: continue
        job_data["logs"].append(ls)
        if ls.startswith("UNSLOTH_TOTAL_STEPS="):
            try: ts=int(ls.split('=')[1])
            except: pass
        if ls.startswith("{{") and ls.endswith("}}"):
            try:
                log=json.loads(ls)
                if "step" in log and ts>0: job_data["progress"]=round((log["step"]/ts)*100,2)
            except: pass
    await proc.wait()
    job_data["status"]="completed" if proc.returncode==0 else "failed"
    job_data["model_path"]=f"/content/trained_models/{job_id}" if proc.returncode==0 else None
    if script_path.exists(): script_path.unlink()

# --- API Endpoints ---
@app.get("/")
async def root():
    return {"message": "Unsloth API is running!"}

@app.get("/models")
async def get_available_models(): return AVAILABLE_MODELS

@app.post("/upload")
async def upload_dataset(file: UploadFile = File(...)):
    if Path(file.filename).suffix.lower() not in [".pdf",".txt",".md"]: raise HTTPException(400, "Unsupported file type.")
    fp=UPLOAD_DIR/file.filename
    with open(fp,"wb") as b: shutil.copyfileobj(file.file,b)
    return {"filename":file.filename}

@app.post("/train", status_code=202)
async def start_training(bgt: BackgroundTasks, model_name: str=Form(...), dataset_file: str=Form(...)):
    if model_name not in AVAILABLE_MODELS: raise HTTPException(400, "Model not available")
    job_id=str(uuid.uuid4())
    training_jobs[job_id]={"job_id":job_id,"status":"pending","model_name":model_name,"dataset_file":dataset_file}
    bgt.add_task(run_training,job_id,training_jobs[job_id])
    return {"job_id":job_id}

@app.get("/status/{job_id}", response_model=TrainingStatus)
async def get_training_status(job_id:str):
    if job_id not in training_jobs: raise HTTPException(404,"Job not found")
    return TrainingStatus(**training_jobs[job_id])

@app.get("/logs/{job_id}")
async def get_training_logs(job_id: str):
    """Retrieves the full logs for a specific training job."""
    if job_id not in training_jobs: raise HTTPException(4_04, "Job not found")
    return {"job_id": job_id, "logs": training_jobs[job_id].get("logs", [])}

@app.post("/generate")
async def generate_response(req:ChatRequest):
    if req.job_id not in training_jobs or training_jobs[req.job_id].get("status")!="completed": raise HTTPException(404,"Trained model not found.")
    if req.job_id not in loaded_models:
        from unsloth import FastLanguageModel
        mp=training_jobs[req.job_id]["model_path"]
        loaded_models[req.job_id]=FastLanguageModel.from_pretrained(model_name=mp,load_in_4bit=True)
    m,t=loaded_models[req.job_id]
    i=t([req.prompt],return_tensors="pt").to("cuda");o=m.generate(**i,max_new_tokens=req.max_new_tokens,use_cache=True)
    return JSONResponse({"response":t.batch_decode(o)[0]})

@app.get("/download/{job_id}")
async def download_model(job_id: str, model_name: str):
    if job_id not in training_jobs or training_jobs[job_id].get("status")!="completed": raise HTTPException(404,"Job not found or not completed.")
    mp=Path(training_jobs[job_id]["model_path"]);zp=ZIPPED_MODELS_DIR/f"{model_name}.zip"
    with zipfile.ZipFile(zp,'w',zipfile.ZIP_DEFLATED) as zf:
        for r,_,fs in os.walk(mp):
            for f in fs: zf.write(os.path.join(r,f),os.path.relpath(os.path.join(r,f),mp))
    return FileResponse(path=zp,media_type='application/zip',filename=f"{model_name}.zip")

# --- Start Server ---
public_url = ngrok.connect(8000)
print(f"âœ… Unsloth API Server is running!\nðŸš€ Public URL: {public_url.public_url}")
uvicorn.run(app, host="0.0.0.0", port=8000)